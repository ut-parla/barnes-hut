[general]
# sequential, singlegpu, multigpu, parla
implementation = parlamgpu

sample_check_size = 10
#if skip_timestep, don't run more than one round, since
#timestep is the step responsible for resetting acc to 0
skip_timestep = True
# Let's run multiple evals on the same
# set of points so we can better measure
# For measuring correctness this value MUST be 1
evaluation_rounds = 1
rounds = 1

[grid]
max_particles_per_box = 1000
box_occupancy = 0.7

[bh]
# 6.673 * math.pow(10, -11)
#grav_constant = 0.0000000001
grav_constant = 1
tick_seconds = 0.005

[cuda]
# TODO: use this
threads_per_block = 128

#
# config belows are implementation specific
[parla]
gpus_available = 2
placement_cpu_tasks = 0
placement_gpu_tasks = 1
summarize_cpu_tasks = 0
summarize_gpu_tasks = 1
evaluation_cpu_tasks = 0
evaluation_gpu_tasks = 1
#grav kernel used will be the sequential,force_calculation
timestep_cpu_tasks = 1
timestep_gpu_tasks = 0

[sequential]
# naive, com_concat, com_concat_dedup
evaluation = com_concat_dedup
# np, numba (np is faster, ignore numba)
grid_placement = np
# guvectorize-cpu, guvectorize-parallel, guvectorize-cuda are broken
# blas is incorrect
# vect is naive baseline
# blas, nop, numba, vect
force_calculation = numba

[singlegpu]
#nothing for now

[multigpu]
ngpus = 1

[pykokkos]
# Cuda, OpenMP
space = Cuda